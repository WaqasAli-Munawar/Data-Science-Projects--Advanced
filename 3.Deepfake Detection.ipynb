{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There have been many reports of fake videos of popular celebrities or politicians. These manipulated videos are created by manipulating original videos. These fake videos are difficult to detect with the naked eye and are becoming a major problem in society.\n",
    "\n",
    "It has been experienced that the Deepfake videos go easily viral on Social media platforms like **Facebook**, **twitter**, **youtube**, etc. They are the main distribution channel for these manipulated videos. While these platforms are working to solve this problem, Facebook is making a large investment ($ 10 million) to address this problem and other platforms like Twitter and Google are also working to solve this problem. These are some [more details](https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html) on Google's fight against deep fakes.\n",
    "\n",
    "Deepfake detection is therefore not an easy task. In this project, we will see how to identify the fakes from the real ones. It includes breaking videos into a frame, face detection from real and fake videos, face cropping and analysis. We will use:\n",
    "\n",
    "1. `OpenCV`: OpenCV supports many algorithms related to computer vision and machine learning. OpenCV has a python library called **OpenCV-Python**.\n",
    "2. `Numpy`: is used for general representation and manipulation of arrays in Python.\n",
    "3. `scikit-image`: We will use this tool for face image operations.\n",
    "4. `Matplotlib`: is used to plot the survey results (histograms andROC curves, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using [the DeepfakeTIMIT database](https://www.idiap.ch/dataset/deepfaketimit) , a video database where faces are swapped using the open source GAN based approach.\n",
    "\n",
    "This dataset was created [by the Deepfake algorithm based on the original autoencoder](https://github.com/deepfakes/faceswap).\n",
    "\n",
    "We will train / test using two different trained GAN models: a lower quality (LQ) with a `64 x 64` input / output size model and a higher quality (HQ) with a `128 x 128` size model.\n",
    "\n",
    "Let's move on to the main topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV\n",
    "\n",
    "OpenCV installation using pip\n",
    "\n",
    "`pip install opencv`\n",
    "\n",
    "Please follow [this](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_setup_in_windows/py_setup_in_windows.html) article:\n",
    "\n",
    "OpenCV will be used for four main activities.\n",
    "\n",
    "* Image processing\n",
    "* Video processing\n",
    "* Feature detection\n",
    "* Object detection\n",
    "\n",
    "To use OpenCV, all we need to do is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import cv2`\n",
    "\n",
    "`# Load an color image in grayscale\n",
    "img = cv2.imread('imagename.jpg',0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `cv2.IMREAD_COLOR`: Upload a color image. Any transparency of the image will be neglected. It is the default flag.\n",
    "* `cv2.IMREAD_GRAYSCALE`: Loads the image in grayscale mode\n",
    "* `cv2.IMREAD_UNCHANGED`: Load image as such, including alpha channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View an image\n",
    "\n",
    "`cv2.imshow('image',img)\n",
    " cv2.waitKey(0)\n",
    " cv2.destroyAllWindows()`\n",
    " \n",
    "### Write an image\n",
    "\n",
    "`cv2.imwrite('imagename.png',img)`\n",
    "\n",
    "### Capture video from the camera\n",
    "\n",
    "`import numpy as np\n",
    " import cv2\n",
    " cap = cv2.VideoCapture(0)`\n",
    "\n",
    "`while(True):\n",
    "    ret, frame = cap.read() # Capture frame-by-frame\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Our operations on the frame come here\n",
    "    cv2.imshow('frame',gray) # Display the resulting frame\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break`\n",
    "\n",
    "`# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will open a video from the camera/Playing a video from a file.\n",
    "\n",
    "`import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture('yourvoideofilename.avi')`\n",
    "\n",
    "`while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break`\n",
    "\n",
    "`cap.release()\n",
    "cv2.destroyAllWindows()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A video file from the path provided will open. These are the general tasks performed in OpenCV, but we can find a lot more [here](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_drawing_functions/py_drawing_functions.html) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "\n",
    "Matplotlib will provide us with a wide variety of printing methods. Here is an example of how to display the image using matplotlib.\n",
    "\n",
    "`import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread('yourimagename.jpg',0)\n",
    "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([]) # to hide tick values on X and Y axis\n",
    "plt.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main goal is to read a video file module path and extract frames from real videos and fake videos of our choice. Then perform face detection to extract faces. Next, we're analyzing the differences in these two face images using various measurements. We are dividing the codes into several sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import glob2\n",
    "import os, fnmatch\n",
    "from pathlib import Path\n",
    "# import mtcnn\n",
    "from mtcnn.mtcnn import MTCNN # from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we have two types of videos, one real and one fake. We want to detect which is the fake one amongst the two. Probably this is what we need to do in the task of Deepfake detection.\n",
    "\n",
    "Now, we will create a function to process both the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading video fame\n",
    "# Create a VideoCapture object and read from input file\n",
    "\n",
    "def extract_multiple_videos(intput_filenames, image_path_infile):\n",
    "    \n",
    "    \"\"\"Extract video files into sequence of images.\"\"\"\n",
    "    \n",
    "    i = 1  # Counter of first video\n",
    "    \n",
    "    # Iterate file names:\n",
    "    cap = cv2.VideoCapture('your_video_file_path.avi' or intput_filenames)\n",
    "    \n",
    "    if (cap.isOpened()== False):\n",
    "            print(\"Error opening file\")\n",
    "    \n",
    "    # Keep iterating break\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Read frame from first video\n",
    "\n",
    "        if ret:\n",
    "            cv2.imwrite(os.path.join(image_path_infile , str(i) + '.jpg'), frame)  # Write frame to JPEG file (1.jpg, 2.jpg, ...)\n",
    "        #  cv2.imshow('frame', frame)  # Display frame for testing. We can uncomment this line if we want to view them.\n",
    "            i += 1 # Advance file counter\n",
    "        else:\n",
    "            # Break the interal loop when res status is False.\n",
    "            break\n",
    "    \n",
    "    cv2.waitKey(50) # Wait 50msec\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_multiple_videos(fake_video_name, fake_image_path_for_frame)\n",
    "# extract_multiple_videos(real_video_name, real_image_path_for_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the function we just uploaded the video, read the frame and wrote to a file.\n",
    "\n",
    "Now we find the difference between the two frames and visualize the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the\n",
    "    # sum of the squared difference between the two images;\n",
    "    # NOTE: the two images must have the same dimension\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    # return the MSE, the lower the error, the more \"similar\"\n",
    "    # the two images are\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(imageA, imageB, title):\n",
    "    # compute the mean squared error and structural similarity\n",
    "    # index for the images\n",
    "    m = mse(imageA, imageB)\n",
    "    s = measure.compare_ssim(imageA, imageB)\n",
    "    # setup the figure\n",
    "    fig = plt.figure(title)\n",
    "    plt.suptitle(\"MSE: %.2f, SSIM: %.2f\" % (m, s))\n",
    "    # show first image\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(imageA, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    # show the second image\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(imageB, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    # show the images\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we are comparing the extracted images from the original video and the corresponding image from fake videos. In the last section of the code, we checked if both the two images have any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = cv2.imread(\"image_path_from_real_video\")\n",
    "shopped = cv2.imread(\"image_path_from_fake_video\")\n",
    "\n",
    "# convert the images to grayscale\n",
    "original = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
    "shopped = cv2.cvtColor(shopped, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the figure\n",
    "fig = plt.figure(\"Images\")\n",
    "images = (\"Original\", original), (\"modified\", shopped)\n",
    "\n",
    "# loop over the images\n",
    "for (i, (name, image)) in enumerate(images):\n",
    "    # show the image\n",
    "    ax = fig.add_subplot(1, 3, i + 1)\n",
    "    ax.set_title(name)\n",
    "    plt.imshow(image, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the images\n",
    "compare_images(original, original, \"Original vs. Original\")\n",
    "compare_images(original, shopped, \"Original vs. Modified\")\n",
    "# cv2.subtract(original)\n",
    "# img3 = original-shopped\n",
    "image3 = cv2.absdiff(original, shopped)\n",
    "image3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_all_zero = not np.any(image3)\n",
    "\n",
    "if is_all_zero:\n",
    "    print('Array contains only 0')\n",
    "else:\n",
    "    print('Array has non-zero items too')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now irritated that those two images are different. We can calculate **SSIM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the structural similarity index (SSIM)\n",
    "\n",
    "from skimage.measure import compare_ssim\n",
    "import argparse\n",
    "import imutils\n",
    "(score, diff) = compare_ssim(original, shopped, full=True)\n",
    "diff = (diff * 255).astype(“uint8”)\n",
    "print(“SSIM: {}”.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the differences in the pictures\n",
    "\n",
    "thresh = cv2.threshold(diff, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline# loop over the contours\n",
    "for c in cnts:\n",
    "    # compute the bounding box of the contour and then draw the\n",
    "    # bounding box on both input images to represent where the two\n",
    "    # images differ\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    cv2.rectangle(original, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    cv2.rectangle(shopped, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "# show the output images\n",
    "# plt.imshow(“Original”, original)\n",
    "plt.imshow(original)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# Modified image\n",
    "# plt.imshow(“Modified”, shopped)\n",
    "plt.imshow(shopped)\n",
    "\n",
    "#Differences in two images\n",
    "# plt.imshow(“diff”, diff)\n",
    "plt.imshow(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the images, we can see what the differences were between the two images and calculate the modified part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "# draw an image with detected objects\n",
    "def draw_image_with_boxes(filename, result_list, face_filename):\n",
    "    \n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    \n",
    "    for i in range(len(result_list)):\n",
    "        # get coordinates\n",
    "        x1, y1, width, height = result_list[i][‘box’]\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        # define subplot\n",
    "        pyplot.subplot(1, len(result_list), i+1)\n",
    "        pyplot.axis(‘off’)\n",
    "        # plot face\n",
    "        pyplot.imshow(data[y1:y2, x1:x2])\n",
    "        pyplot.savefig(‘Dataset/only_face/’+ face_filename)\n",
    "\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading video fame\n",
    "# Create a VideoCapture object and read from input file\n",
    "\n",
    "def extract_multiple_videos_faces(intput_video_file_names, image_path_infile):\n",
    "    \"\"\"Extract video files into sequence of images.\n",
    "       Intput_filenames is a list for video file names\"\"\"\n",
    "    i = 1  # Counter of first video\n",
    "    \n",
    "    # Iterate file names:\n",
    "    cap = cv2.VideoCapture('/Users/praladneupane/Documents/Third_Semester/695/projects/Deepfake/Dataset/VidTIMIT/mstk0/sa1.avi')\n",
    "        \n",
    "    if (cap.isOpened()== False):\n",
    "        print(\"Error opening video stream or file\")\n",
    "    \n",
    "    # Keep iterating break\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Read frame from first video\n",
    "            \n",
    "        if ret:\n",
    "            \n",
    "#           cv2.imwrite(str(i) + '.jpg', frame)  # Write frame to JPEG file (1.jpg, 2.jpg, ...)\n",
    "#             this code can be use to do 1.2.3 and 1.3, but i am only doing 1.3 now\n",
    "#             cv2.imwrite(os.path.join(image_path_infile , str(i) + '.jpg'), frame)  # Write frame to JPEG file (1.jpg, 2.jpg, ...)\n",
    "                \n",
    "#           cv2.imshow('frame', frame)  # Display frame for testing\n",
    "    \n",
    "            filename = os.path.join(image_path_infile , str(i) + '.jpg') \n",
    "        \n",
    "        # this line feels little odd. Cause it looks \n",
    "        # like i am reading it mannually but if i uncomment above line it will be dynamic everytime, cause those line \n",
    "        # creates the frame in the folder first.\n",
    "        \n",
    "            # load image from file\n",
    "            pixels = pyplot.imread(filename)\n",
    "            # create the detector, using default weights\n",
    "            detector = MTCNN()\n",
    "            # detect faces in the image\n",
    "            faces = detector.detect_faces(pixels)\n",
    "            # display faces on the original image\n",
    "            face_filename_crp = str(i) + '.jpg'\n",
    "            draw_image_with_boxes(filename, faces, face_filename_crp)\n",
    "        \n",
    "    \n",
    "            i += 1 # Advance file counter\n",
    "        \n",
    "            face_filename = str(i) + '.jpg'\n",
    "            # display faces on the original image\n",
    "            # draw_image_with_boxes(filename, faces,face_filename)\n",
    "        else:\n",
    "            # Break the interal loop when res status is False.\n",
    "            break\n",
    "    cv2.waitKey(100) #Wait 100msec (for debugging)\n",
    "    cap.release() #Release must be inside the outer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_multiple_videos_faces(video_file_path,real_image_path_for_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the unique face images both real and fake. We can try to implement the above section called **Find Outlines** . It was intentional, so give it a try.\n",
    "\n",
    "In conclusion, we are able to extract frames from real and fake videos, compare and contrast the same image from both and detect the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
