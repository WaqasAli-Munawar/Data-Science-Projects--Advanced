{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we will work on how we can classify the nationalities of people by using their names. There is a lot about how we can play with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_url = \"https://raw.githubusercontent.com/amankharwal/Website-data/master/Indian-Female-Names.csv\"\n",
    "# m_url = \"https://raw.githubusercontent.com/amankharwal/Website-data/master/Indian-Male-Names.csv\"\n",
    "\n",
    "# male_data = pd.read_csv(m_url)\n",
    "# female_data = pd.read_csv(f_url)\n",
    "\n",
    "male_data = pd.read_csv(\"nationality/male_names.csv\")\n",
    "female_data = pd.read_csv(\"nationality/female_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shivani</td>\n",
       "      <td>f</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isha</td>\n",
       "      <td>f</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smt shyani devi</td>\n",
       "      <td>f</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>divya</td>\n",
       "      <td>f</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mansi</td>\n",
       "      <td>f</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name gender    race\n",
       "0          shivani      f  indian\n",
       "1             isha      f  indian\n",
       "2  smt shyani devi      f  indian\n",
       "3            divya      f  indian\n",
       "4            mansi      f  indian"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating helper functions for data cleaning and processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_list = ['s/o','d/o','w/o','/','&',',','-']\n",
    "\n",
    "def clean_data(name):\n",
    "    name = str(name).lower()\n",
    "    name = (''.join(i for i in name if ord(i)<128)).strip()\n",
    "    for repl in repl_list:\n",
    "        name = name.replace(repl,\" \")\n",
    "    if '@' in name:\n",
    "        pos = name.find('@')\n",
    "        name = name[:pos].strip()\n",
    "    name = name.split(\" \")\n",
    "    name = \" \".join([each.strip() for each in name])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_records(merged_data):\n",
    "    merged_data['delete'] = 0\n",
    "    merged_data.loc[merged_data['name'].str.find('with') != -1,'delete'] = 1\n",
    "    merged_data.loc[merged_data['count_words']>=5,'delete']=1\n",
    "    merged_data.loc[merged_data['count_words']==0,'delete']=1\n",
    "    merged_data.loc[merged_data['name'].str.contains(r'\\d') == True,'delete']=1\n",
    "    cleaned_data = merged_data[merged_data.delete==0]\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.concat((male_data,female_data),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['name'] = merged_data['name'].apply(clean_data)\n",
    "merged_data['count_words'] = merged_data['name'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = remove_records(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_cleaned_data = cleaned_data[['name','count_words']].drop_duplicates(subset='name',keep='first')\n",
    "indian_cleaned_data['label'] = 'indian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13754"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indian_cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading and removing the wrong entries in the data, we got a few records around 13,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>count_words</th>\n",
       "      <th>delete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barjraj</td>\n",
       "      <td>m</td>\n",
       "      <td>indian</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ramdin verma</td>\n",
       "      <td>m</td>\n",
       "      <td>indian</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sharat chandran</td>\n",
       "      <td>m</td>\n",
       "      <td>indian</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>birender mandal</td>\n",
       "      <td>m</td>\n",
       "      <td>indian</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amit</td>\n",
       "      <td>m</td>\n",
       "      <td>indian</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name gender    race  count_words  delete\n",
       "0          barjraj      m  indian            1       0\n",
       "1     ramdin verma      m  indian            2       0\n",
       "2  sharat chandran      m  indian            2       0\n",
       "3  birender mandal      m  indian            2       0\n",
       "4             amit      m  indian            1       0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indian    30227\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barjraj</td>\n",
       "      <td>1</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ramdin verma</td>\n",
       "      <td>2</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sharat chandran</td>\n",
       "      <td>2</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>birender mandal</td>\n",
       "      <td>2</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amit</td>\n",
       "      <td>1</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name  count_words   label\n",
       "0          barjraj            1  indian\n",
       "1     ramdin verma            2  indian\n",
       "2  sharat chandran            2  indian\n",
       "3  birender mandal            2  indian\n",
       "4             amit            1  indian"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some non-Indian names using Faker - a pretty cool package to generate realistic names from different regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James Lewis'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "fake = Faker(\"en_US\")\n",
    "fake.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "req = 15000\n",
    "non_indian_names = []\n",
    "\n",
    "langs = ['ar_EG','bs_BA','de_DE','dk_DK','en_AU','en_CA','en_GB',\n",
    "'en_IN','en_NZ','en_US','it_IT','no_NO','ro_RO']\n",
    "\n",
    "for i in range(0,req):\n",
    "    lng_indx = random.randint(0,len(langs)-1)\n",
    "    fake = Faker(langs[lng_indx])\n",
    "    non_indian_names.append(fake.name().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_indian_names_orig = list(set(non_indian_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_indian_names_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_indian_data = pd.DataFrame({'name':non_indian_names_orig})\n",
    "non_indian_data['count_words'] = non_indian_data['name'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated approximately the same number of names as we have in the Indian data set. We then removed samples longer than 5 words. The Indian data set contained a lot of names with just first names. So we need to make the overall non-Indian distribution also similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_indian_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the distribution of count of words in names. We dont want them to be too different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_cleaned_data['count_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_indian_data['count_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_word_names = non_indian_data[non_indian_data['count_words']==2]['name']\n",
    "one_word_req = 5000\n",
    "names_one_two_words = [each.split()[0] for each in two_word_names[:one_word_req]] + list(two_word_names[one_word_req:])\n",
    "count_words = [1] * one_word_req + [2] * len(two_word_names[one_word_req:])\n",
    "not_two_words_pd  = non_indian_data[non_indian_data['count_words']!=2]\n",
    "one_two_words_pd = pd.DataFrame({'name':names_one_two_words,'count_words':count_words})\n",
    "non_indian_data = pd.concat((not_two_words_pd,one_two_words_pd),axis=0)\n",
    "non_indian_data['count_words'].value_counts()\n",
    "non_indian_data['label'] = 'non_indian'\n",
    "non_indian_data = non_indian_data[non_indian_data['count_words']<5]\n",
    "non_indian_data['count_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat((non_indian_data[['name','label']],indian_cleaned_data[['name','label']]),axis=0)\n",
    "name_data = full_data.sample(frac=1)\n",
    "\n",
    "# full_data.to_csv(\"name_data.csv\",index=False)\n",
    "# from google.colab import files\n",
    "# files.download('name_data.csv')\n",
    "\n",
    "name_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_url = \"https://raw.githubusercontent.com/ashavish/name-nationality/master/data/name_data.csv\"\n",
    "# name_data = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with about 14,000 non-Indian names and 13,000 Indian names. Now let’s build a neural network to classify nationalities using names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = name_data['name'].astype(str)\n",
    "Y = name_data['label']\n",
    "train_names,test_names,train_labels,test_labels = train_test_split(X,Y,test_size=0.20,random_state =42,stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Count Vectorizer for name classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_ = vectorizer.fit_transform(train_names.values.astype('U'))\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_,train_labels)\n",
    "\n",
    "X_test = vectorizer.transform(test_names.values.astype('U'))\n",
    "\n",
    "test_predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(test_labels,test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on new Names\n",
    "Lets create some names which are not present in the data at all and check the model on these names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_new_names = ['lalitha','tyson','shailaja','shyamala','vishwanathan','ramanujam','conan','kryslovsky',\n",
    "'ratnani','diego','kakoli','shreyas','brayden','shanon']\n",
    "\n",
    "X_new = vectorizer.transform(check_new_names)\n",
    "predictions_nb_cv = model.predict(X_new)\n",
    "test = pd.DataFrame({'names':check_new_names,'predictions_nb_cv':predictions_nb_cv}) \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesnt do well at all ! But thats expected. Now lets try with subword encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with SentencePiece Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip3 install tokenizers\n",
    "from tokenizers import ByteLevelBPETokenizer,CharBPETokenizer,SentencePieceBPETokenizer,BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "f = open(\"train_names.txt\",\"w\")\n",
    "for each in list(train_names):\n",
    "    f.write(str(each))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train([\"./train_names.txt\"],vocab_size=2000,min_frequency=2)\n",
    "\n",
    "encoded_tokens = [tokenizer.encode(str(each)).tokens for each in train_names]\n",
    "encoded_tokens_test = [tokenizer.encode(str(each)).tokens for each in test_names]\n",
    "\n",
    "encoded_tokens = [\" \".join(each)  for each in encoded_tokens]\n",
    "encoded_tokens_test = [\" \".join(each)  for each in encoded_tokens_test]\n",
    "\n",
    "encoded_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "X_ = tfidf_vect.fit_transform(encoded_tokens)\n",
    "len(tfidf_vect.get_feature_names())\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_,train_labels)\n",
    "\n",
    "X_test = tfidf_vect.transform(encoded_tokens_test)\n",
    "\n",
    "test_predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(test_labels,test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty decent. Now lets check on some new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens_check = [tokenizer.encode(str(each).lower()).tokens for each in check_new_names]\n",
    "encoded_tokens_check = [\" \".join(each)  for each in encoded_tokens_check]\n",
    "\n",
    "X_new = tfidf_vect.transform(encoded_tokens_check)\n",
    "predictions_nb_enc_tf = model.predict(X_new)\n",
    "test = pd.DataFrame({'names':check_new_names,'predictions_nb_enc_tf':predictions_nb_enc_tf}) \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character based encoding with LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import Callback\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_encoded_representation(data,tokenizer,vocab_size,max_len):\n",
    "    char_index_sentences = tokenizer.texts_to_sequences(data)\n",
    "    sequences = [to_categorical(x, num_classes=vocab_size) for x in char_index_sentences]\n",
    "    X = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(str(each)) for each in train_names])\n",
    "# mapping = get_char_mapping(train_names)\n",
    "# vocab_size = len(mapping)\n",
    "\n",
    "tok = Tokenizer(char_level=True)\n",
    "tok.fit_on_texts(train_names)\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "X_train = char_encoded_representation(train_names,tok,vocab_size,max_len)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = char_encoded_representation(test_names,tok,vocab_size,max_len)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "y_train = le.transform(train_labels)\n",
    "y_test = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Specification\n",
    "\n",
    "def build_model(hidden_units,max_len,vocab_size):\n",
    "    model = Sequential()\n",
    "    # model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "    model.add(LSTM(hidden_units,input_shape=(max_len,vocab_size)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(Callback): \n",
    "    def __init__(self,X_test,y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        loss,acc = model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(100,max_len,vocab_size)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64,callbacks=myCallback(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = char_encoded_representation(check_new_names,tok,vocab_size,max_len)\n",
    "\n",
    "predictions_prob = model.predict(X_predict)\n",
    "predictions = np.array(predictions_prob)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions = np.squeeze(predictions)\n",
    "predictions_lstm_char = le.inverse_transform(list(predictions.astype(int)))\n",
    "test = pd.DataFrame({'names':check_new_names,'predictions_lstm_char':predictions_lstm_char}) \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Encoding with LSTM\n",
    "\n",
    "Lets also check with a encoding using the **SentencePiece Encoding** we used for Naive Bayes. But now we will use it with an LSTM with a much smaller vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer,CharBPETokenizer,SentencePieceBPETokenizer,BertWordPieceTokenizer\n",
    "vocab_size = 200\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train([\"./train_names.txt\"],vocab_size=vocab_size,min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_piece_encoded_representation(data,tokenizer):\n",
    "    encoded_tokens = [tokenizer.encode(str(each)).ids for each in data]\n",
    "    sequences = [to_categorical(x, num_classes=vocab_size) for x in encoded_tokens]\n",
    "    X = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(str(each)) for each in train_names])\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "y_train = le.transform(train_labels)\n",
    "y_test = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent_piece_encoded_representation(train_names,tokenizer)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sent_piece_encoded_representation(test_names,tokenizer)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(100,max_len,vocab_size)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64,callbacks=myCallback(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = sent_piece_encoded_representation(check_new_names,tokenizer)\n",
    "\n",
    "predictions_prob = model.predict(X_predict)\n",
    "predictions = np.array(predictions_prob)\n",
    "predictions[np.where(predictions > 0.5)[0]] = 1\n",
    "predictions[np.where(predictions <= 0.5)[0]] = 0\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "predictions_lstm_sent_enc = le.inverse_transform(list(predictions.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({'names':check_new_names,'predictions_lstm_sent_enc':predictions_lstm_sent_enc}) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
